{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Version: 2.1.6\n",
      "Tensorflow Version: 1.9.0\n",
      "image dim ordering: tf\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "print(\"Keras Version:\", keras.__version__)\n",
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "print(\"image dim ordering:\", K.image_dim_ordering())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# from tankbuster.cnn import CNNArchitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"data_reduced/\"  # Input directory\n",
    "learning_rate = 0.001  # Learning rate\n",
    "opt = \"SGD\"  # Optimizer\n",
    "target_size = (150, 150)  # Target size for data augmentation\n",
    "\n",
    "input_shape = (150, 150,3)\n",
    "# Configure a callback to reduce the learning rate upon plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=50,\n",
    "                              cooldown=50, min_lr=0.0001, verbose=1)\n",
    "\n",
    "# Path to pre-trained weights file, if used. Otherwise None.\n",
    "weights = None\n",
    "\n",
    "# Configure the validation parameters\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "# Read data & labels\n",
    "data, labels, classes = [], [], {}\n",
    "\n",
    "for (root, subdirs, files) in os.walk(input_dir):\n",
    "    # Assign a numerical identifier to each class directory\n",
    "    for i, class_dir in enumerate(subdirs):\n",
    "        classes[class_dir] = i\n",
    "\n",
    "    # Define allowed image extensions\n",
    "    ext = ['png', 'jpg', 'jpeg']\n",
    "\n",
    "    # Loop over the files in each directory\n",
    "    for f in files:\n",
    "        if f.split('.')[-1] in ext:  # Check file extension\n",
    "            path = os.path.join(root, f)  # Get image path\n",
    "            label = path.split('/')[-2]  # Extract class label from path\n",
    "            numlabel = classes[label]  # Get numerical label from the dict\n",
    "\n",
    "#             print \"Now processing {}/{}/{}\".format(path,label, numlabel)\n",
    "\n",
    "            # Load and preprocess image\n",
    "            image = load_img(path, target_size=target_size)  # Load image\n",
    "            features = img_to_array(image)  # Convert image to numpy array\n",
    "\n",
    "            labels.append(numlabel)  # Append label to list\n",
    "            data.append(features) # Append features to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data and labels to numpy arrays\n",
    "data = np.asarray(data, dtype=np.float32)\n",
    "labels = np.asarray(labels, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 150, 150, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.layers import Flatten, Input, Dense, Dropout, Conv2D,MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "256/256 [==============================] - 65s 255ms/step - loss: 0.6132 - acc: 0.6382 - val_loss: 0.6018 - val_acc: 0.6261\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 62s 241ms/step - loss: 0.5248 - acc: 0.7676 - val_loss: 0.4999 - val_acc: 0.7441\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 62s 243ms/step - loss: 0.4680 - acc: 0.7891 - val_loss: 0.4958 - val_acc: 0.7462\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 64s 249ms/step - loss: 0.4329 - acc: 0.8130 - val_loss: 0.4854 - val_acc: 0.7457\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 65s 254ms/step - loss: 0.4216 - acc: 0.8223 - val_loss: 0.4787 - val_acc: 0.7733\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 62s 241ms/step - loss: 0.4094 - acc: 0.8247 - val_loss: 0.4738 - val_acc: 0.7592\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 63s 246ms/step - loss: 0.4025 - acc: 0.8247 - val_loss: 0.4706 - val_acc: 0.7889\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 62s 244ms/step - loss: 0.3924 - acc: 0.8291 - val_loss: 0.4609 - val_acc: 0.7598\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 62s 242ms/step - loss: 0.3750 - acc: 0.8350 - val_loss: 0.4605 - val_acc: 0.7598\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 63s 245ms/step - loss: 0.3626 - acc: 0.8384 - val_loss: 0.4488 - val_acc: 0.7608\n",
      "75/75 [==============================] - 1s 7ms/step\n",
      "Epoch 1/10\n",
      "256/256 [==============================] - 66s 256ms/step - loss: 0.6124 - acc: 0.6284 - val_loss: 0.5997 - val_acc: 0.6687\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 61s 238ms/step - loss: 0.5300 - acc: 0.7593 - val_loss: 0.5160 - val_acc: 0.7738\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 62s 241ms/step - loss: 0.4588 - acc: 0.7944 - val_loss: 0.5001 - val_acc: 0.7603\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 63s 248ms/step - loss: 0.4439 - acc: 0.8042 - val_loss: 0.4767 - val_acc: 0.7868\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 64s 250ms/step - loss: 0.4280 - acc: 0.8120 - val_loss: 0.4700 - val_acc: 0.7884\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 64s 249ms/step - loss: 0.4217 - acc: 0.8125 - val_loss: 0.4567 - val_acc: 0.7873\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 66s 260ms/step - loss: 0.4093 - acc: 0.8193 - val_loss: 0.4526 - val_acc: 0.8008\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 65s 256ms/step - loss: 0.3973 - acc: 0.8232 - val_loss: 0.4461 - val_acc: 0.8014\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 63s 248ms/step - loss: 0.3962 - acc: 0.8184 - val_loss: 0.4478 - val_acc: 0.7868\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 82s 321ms/step - loss: 0.3749 - acc: 0.8315 - val_loss: 0.4327 - val_acc: 0.7993\n",
      "75/75 [==============================] - 1s 11ms/step\n",
      "Epoch 1/10\n",
      "256/256 [==============================] - 100s 391ms/step - loss: 0.6103 - acc: 0.6338 - val_loss: 0.5638 - val_acc: 0.7062\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 98s 382ms/step - loss: 0.5288 - acc: 0.7612 - val_loss: 0.4564 - val_acc: 0.7868\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 102s 398ms/step - loss: 0.4524 - acc: 0.8140 - val_loss: 0.4367 - val_acc: 0.8008\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 100s 390ms/step - loss: 0.4341 - acc: 0.8115 - val_loss: 0.4308 - val_acc: 0.7993\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 97s 379ms/step - loss: 0.4252 - acc: 0.8130 - val_loss: 0.4105 - val_acc: 0.8128\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 96s 374ms/step - loss: 0.4045 - acc: 0.8237 - val_loss: 0.4138 - val_acc: 0.8128\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 99s 387ms/step - loss: 0.3907 - acc: 0.8301 - val_loss: 0.3936 - val_acc: 0.8144\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 100s 389ms/step - loss: 0.3817 - acc: 0.8272 - val_loss: 0.3755 - val_acc: 0.8123\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 103s 403ms/step - loss: 0.3643 - acc: 0.8306 - val_loss: 0.3712 - val_acc: 0.8123\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 98s 382ms/step - loss: 0.3380 - acc: 0.8433 - val_loss: 0.3645 - val_acc: 0.8263\n",
      "75/75 [==============================] - 1s 13ms/step\n",
      "Epoch 1/10\n",
      "256/256 [==============================] - 101s 393ms/step - loss: 0.6131 - acc: 0.6231 - val_loss: 0.5902 - val_acc: 0.6667\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 96s 373ms/step - loss: 0.5345 - acc: 0.7505 - val_loss: 0.4910 - val_acc: 0.7868\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 98s 385ms/step - loss: 0.4717 - acc: 0.7993 - val_loss: 0.4659 - val_acc: 0.7738\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 98s 384ms/step - loss: 0.4350 - acc: 0.8096 - val_loss: 0.4497 - val_acc: 0.7868\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 97s 378ms/step - loss: 0.4353 - acc: 0.8076 - val_loss: 0.4341 - val_acc: 0.8268\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 97s 380ms/step - loss: 0.4210 - acc: 0.8159 - val_loss: 0.4206 - val_acc: 0.8284\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 97s 379ms/step - loss: 0.3999 - acc: 0.8242 - val_loss: 0.4162 - val_acc: 0.8263\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 98s 382ms/step - loss: 0.3947 - acc: 0.8232 - val_loss: 0.4032 - val_acc: 0.8253\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 99s 386ms/step - loss: 0.3796 - acc: 0.8271 - val_loss: 0.4000 - val_acc: 0.8268\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 97s 378ms/step - loss: 0.3708 - acc: 0.8252 - val_loss: 0.3912 - val_acc: 0.8268\n",
      "75/75 [==============================] - 1s 11ms/step\n",
      "Epoch 1/10\n",
      "256/256 [==============================] - 100s 391ms/step - loss: 0.6189 - acc: 0.6255 - val_loss: 0.5774 - val_acc: 0.6623\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 97s 380ms/step - loss: 0.5181 - acc: 0.7568 - val_loss: 0.4781 - val_acc: 0.7682\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 98s 382ms/step - loss: 0.4563 - acc: 0.8003 - val_loss: 0.4722 - val_acc: 0.7698\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 98s 382ms/step - loss: 0.4373 - acc: 0.8125 - val_loss: 0.4554 - val_acc: 0.7708\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 64s 248ms/step - loss: 0.4241 - acc: 0.8154 - val_loss: 0.4564 - val_acc: 0.7692\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 62s 242ms/step - loss: 0.4174 - acc: 0.8218 - val_loss: 0.4555 - val_acc: 0.7692\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 63s 246ms/step - loss: 0.4119 - acc: 0.8232 - val_loss: 0.4493 - val_acc: 0.7708\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 62s 242ms/step - loss: 0.3954 - acc: 0.8272 - val_loss: 0.4461 - val_acc: 0.7698\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 64s 251ms/step - loss: 0.3886 - acc: 0.8257 - val_loss: 0.4317 - val_acc: 0.7993\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 64s 249ms/step - loss: 0.3718 - acc: 0.8330 - val_loss: 0.4298 - val_acc: 0.7835\n",
      "74/74 [==============================] - 1s 8ms/step\n",
      "Epoch 1/10\n",
      "256/256 [==============================] - 66s 259ms/step - loss: 0.6183 - acc: 0.6162 - val_loss: 0.5800 - val_acc: 0.7013\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 64s 251ms/step - loss: 0.5396 - acc: 0.7441 - val_loss: 0.4861 - val_acc: 0.7835\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 63s 247ms/step - loss: 0.4646 - acc: 0.7993 - val_loss: 0.4664 - val_acc: 0.7835\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 64s 249ms/step - loss: 0.4370 - acc: 0.8076 - val_loss: 0.4490 - val_acc: 0.7961\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 63s 247ms/step - loss: 0.4337 - acc: 0.8081 - val_loss: 0.4442 - val_acc: 0.7966\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 63s 247ms/step - loss: 0.4151 - acc: 0.8179 - val_loss: 0.4267 - val_acc: 0.8114\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 61s 239ms/step - loss: 0.4082 - acc: 0.8184 - val_loss: 0.4340 - val_acc: 0.8240\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 62s 242ms/step - loss: 0.3962 - acc: 0.8252 - val_loss: 0.4071 - val_acc: 0.7972\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 63s 246ms/step - loss: 0.3914 - acc: 0.8237 - val_loss: 0.3997 - val_acc: 0.8114\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 63s 245ms/step - loss: 0.3684 - acc: 0.8340 - val_loss: 0.3888 - val_acc: 0.8109\n",
      "74/74 [==============================] - 1s 7ms/step\n",
      "Epoch 1/10\n",
      "256/256 [==============================] - 66s 256ms/step - loss: 0.6091 - acc: 0.6310 - val_loss: 0.5749 - val_acc: 0.7261\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 62s 243ms/step - loss: 0.5165 - acc: 0.7687 - val_loss: 0.4750 - val_acc: 0.7960\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 62s 243ms/step - loss: 0.4554 - acc: 0.8019 - val_loss: 0.4509 - val_acc: 0.7939\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 63s 245ms/step - loss: 0.4319 - acc: 0.8135 - val_loss: 0.4349 - val_acc: 0.8078\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 63s 245ms/step - loss: 0.4151 - acc: 0.8161 - val_loss: 0.4293 - val_acc: 0.7950\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 63s 244ms/step - loss: 0.3943 - acc: 0.8239 - val_loss: 0.4475 - val_acc: 0.7944\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 63s 245ms/step - loss: 0.3890 - acc: 0.8260 - val_loss: 0.4219 - val_acc: 0.7928\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 62s 244ms/step - loss: 0.3740 - acc: 0.8276 - val_loss: 0.4201 - val_acc: 0.7939\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 63s 245ms/step - loss: 0.3523 - acc: 0.8403 - val_loss: 0.4290 - val_acc: 0.7939\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 63s 244ms/step - loss: 0.3393 - acc: 0.8403 - val_loss: 0.3788 - val_acc: 0.8217\n",
      "73/73 [==============================] - 0s 7ms/step\n",
      "Epoch 1/10\n",
      "256/256 [==============================] - 65s 255ms/step - loss: 0.6150 - acc: 0.6257 - val_loss: 0.5678 - val_acc: 0.7261\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 63s 245ms/step - loss: 0.5299 - acc: 0.7594 - val_loss: 0.4501 - val_acc: 0.7822\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 63s 246ms/step - loss: 0.4648 - acc: 0.7944 - val_loss: 0.4115 - val_acc: 0.8078\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 62s 241ms/step - loss: 0.4449 - acc: 0.8047 - val_loss: 0.3934 - val_acc: 0.8227\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 62s 244ms/step - loss: 0.4310 - acc: 0.8047 - val_loss: 0.3898 - val_acc: 0.8761\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 63s 245ms/step - loss: 0.4199 - acc: 0.8185 - val_loss: 0.3704 - val_acc: 0.8777\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 63s 246ms/step - loss: 0.4120 - acc: 0.8141 - val_loss: 0.3673 - val_acc: 0.8628\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 63s 246ms/step - loss: 0.3947 - acc: 0.8239 - val_loss: 0.3481 - val_acc: 0.8772\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 62s 242ms/step - loss: 0.3836 - acc: 0.8257 - val_loss: 0.3369 - val_acc: 0.8911\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 63s 247ms/step - loss: 0.3766 - acc: 0.8252 - val_loss: 0.3220 - val_acc: 0.8895\n",
      "73/73 [==============================] - 0s 6ms/step\n",
      "Epoch 1/10\n",
      "256/256 [==============================] - 65s 254ms/step - loss: 0.6147 - acc: 0.6314 - val_loss: 0.5824 - val_acc: 0.6994\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 63s 245ms/step - loss: 0.5177 - acc: 0.7632 - val_loss: 0.4860 - val_acc: 0.8083\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 62s 243ms/step - loss: 0.4615 - acc: 0.7853 - val_loss: 0.4610 - val_acc: 0.8067\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 62s 242ms/step - loss: 0.4479 - acc: 0.8003 - val_loss: 0.4528 - val_acc: 0.8062\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 63s 245ms/step - loss: 0.4303 - acc: 0.8088 - val_loss: 0.4419 - val_acc: 0.8222\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 63s 245ms/step - loss: 0.4149 - acc: 0.8219 - val_loss: 0.4392 - val_acc: 0.8211\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 63s 245ms/step - loss: 0.4159 - acc: 0.8184 - val_loss: 0.4300 - val_acc: 0.8201\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 62s 243ms/step - loss: 0.4038 - acc: 0.8127 - val_loss: 0.4213 - val_acc: 0.8227\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 63s 247ms/step - loss: 0.3862 - acc: 0.8273 - val_loss: 0.4190 - val_acc: 0.8206\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 63s 246ms/step - loss: 0.3788 - acc: 0.8285 - val_loss: 0.4082 - val_acc: 0.8222\n",
      "73/73 [==============================] - 0s 6ms/step\n",
      "Epoch 1/10\n",
      "256/256 [==============================] - 66s 257ms/step - loss: 0.6216 - acc: 0.6209 - val_loss: 0.5857 - val_acc: 0.7250\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 62s 243ms/step - loss: 0.5164 - acc: 0.7611 - val_loss: 0.4735 - val_acc: 0.7939\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 62s 242ms/step - loss: 0.4675 - acc: 0.7878 - val_loss: 0.4507 - val_acc: 0.8222\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 62s 243ms/step - loss: 0.4510 - acc: 0.8032 - val_loss: 0.4392 - val_acc: 0.8089\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 62s 242ms/step - loss: 0.4335 - acc: 0.8086 - val_loss: 0.4285 - val_acc: 0.8361\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 62s 242ms/step - loss: 0.4277 - acc: 0.8133 - val_loss: 0.4172 - val_acc: 0.8345\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 63s 245ms/step - loss: 0.4153 - acc: 0.8125 - val_loss: 0.4126 - val_acc: 0.8361\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 63s 248ms/step - loss: 0.4100 - acc: 0.8179 - val_loss: 0.4055 - val_acc: 0.8510\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 62s 243ms/step - loss: 0.3924 - acc: 0.8280 - val_loss: 0.4013 - val_acc: 0.8356\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 63s 244ms/step - loss: 0.3802 - acc: 0.8268 - val_loss: 0.3968 - val_acc: 0.8345\n",
      "73/73 [==============================] - 1s 7ms/step\n"
     ]
    }
   ],
   "source": [
    " for n, (train, test) in enumerate(kfold.split(data, labels)):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(2, activation=tf.nn.softmax))\n",
    "        \n",
    "        adam_opt = Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-5)\n",
    "\n",
    "        model.compile(optimizer = adam_opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        traindata, trainlabels = data[train], labels[train]\n",
    "        testdata, testlabels = data[test], labels[test]\n",
    "\n",
    "        # Convert integer labels into one-hot encoded vectors\n",
    "        trainlabels = np_utils.to_categorical(trainlabels, 2)\n",
    "        testlabels = np_utils.to_categorical(testlabels, 2)\n",
    "        \n",
    "        \n",
    "        training_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "        # Generate training data\n",
    "        training_data = training_generator.flow(traindata, trainlabels, batch_size=8)\n",
    "    \n",
    "        # Set up generator for validation data\n",
    "        validation_generator = ImageDataGenerator(rescale=1./255 )\n",
    "\n",
    "        # Generate validation data\n",
    "        validation_data = validation_generator.flow(testdata,testlabels,batch_size=8)\n",
    "\n",
    "        # Start training the model\n",
    "        training = model.fit_generator(training_data,\n",
    "                                       steps_per_epoch=256,\n",
    "                                       epochs=10,\n",
    "                                       validation_data=validation_data,\n",
    "                                       validation_steps=256,\n",
    "                                       callbacks=[reduce_lr])\n",
    "\n",
    "        # Evaluate the model\n",
    "        (loss, accuracy) = model.evaluate(testdata,\n",
    "                                          testlabels,\n",
    "                                          batch_size=8,\n",
    "                                          verbose=1)\n",
    "\n",
    "        # Save weights for each fold into a file\n",
    "#         model.save_weights('{}-cv-fold_{}.h5'.format(arch, (n + 1)),\n",
    "#                            overwrite=True)\n",
    "\n",
    "        # Write the training history for each fold into a file\n",
    "#         with open('{}-history-fold_{}.pkl'.format(arch, (n + 1)), 'wb') \\\n",
    "#                 as histfile:\n",
    "#             pickle.dump(training.history, histfile)\n",
    "\n",
    "        # Append the accuracy to the list of scores\n",
    "        scores.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Mean: 0.7202 and (STDEV 0.0310)\n",
      "Best result for fold 2\n",
      "Best accuracy is 0.76\n"
     ]
    }
   ],
   "source": [
    "# Print the scores and the best fold\n",
    "print (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores), np.std(scores)))\n",
    "print (\"Best result for fold %s\" % np.argmax(scores))\n",
    "print (\"Best accuracy is\", (scores[np.argmax(scores)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6933333333333334,\n",
       " 0.72,\n",
       " 0.76,\n",
       " 0.72,\n",
       " 0.7432432432432432,\n",
       " 0.7297297297297297,\n",
       " 0.7397260273972602,\n",
       " 0.684931506849315,\n",
       " 0.6575342465753424,\n",
       " 0.7534246575342466]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
